{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMI-FastGRNN Implementation\n",
    "This notebook contains the implementation of EMI-FastGRNN (one among the EMI algorithms). \n",
    "This notebook consists of four parts:\n",
    "1. The constructing of an entirely new graph with randomly initialized weights, that can be trained.\n",
    "2. A saved model/graph can be loaded using *EMI-Driver*.\n",
    "3. New graph created using the weights from a saved graph.\n",
    "4. (Experimental)Restoration of weights and biases from numpy matrices\n",
    "\n",
    "This notebook uses the features extracted from UCF_Crime dataset and is an extension of the notebook found in https://github.com/microsoft/EdgeML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:51:58.751435Z",
     "start_time": "2019-04-30T09:51:57.442626Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# To include edgeml in python path\n",
    "sys.path.insert(0, '../../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "\n",
    "# MI-RNN and EMI-RNN imports\n",
    "from rnn import EMI_DataPipeline\n",
    "from rnn import EMI_FastGRNN,EMI_FastRNN\n",
    "from emirnnTrainer import EMI_Trainer, EMI_Driver\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set up some network parameters for the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T09:37:09.981820Z",
     "start_time": "2019-07-28T09:37:09.975515Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_HIDDEN = 128\n",
    "NUM_TIMESTEPS = 64\n",
    "NUM_FEATS = 540\n",
    "FORGET_BIAS = 1.0\n",
    "NUM_OUTPUT = 9\n",
    "USE_DROPOUT = 0\n",
    "\n",
    "KEEP_PROB = 0.9\n",
    "UPDATE_NL = \"quantTanh\"\n",
    "GATE_NL = \"quantSigm\"\n",
    "WRANK = 5\n",
    "URANK = 6\n",
    "PREFETCH_NUM = 5\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "NUM_ITER = 4\n",
    "NUM_ROUNDS = 10\n",
    "\n",
    "# A staging direcory to store models\n",
    "MODEL_PREFIX = '/home/adithyapa4444_gmail_com/fastgrnn_with_accidents/model-fastgrnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emirnn_preprocess(bagsize,no_of_features,extractedDir,numClass,subinstanceLen, subinstanceStride,data_csv,raw_create):\n",
    "    \"\"\"\n",
    "    emirnn_preprocess(bagsize,no_of_features,extractedDir,numClass,subinstanceLen, subinstanceStride,data_csv,raw_create)\n",
    "    \n",
    "    extractedDir: location where the raw file and output models have to be stored\n",
    "    \n",
    "    data_csv: the extracted hof file.\n",
    "    \n",
    "    raw_create: 1 if  raw file already exists in directory \n",
    "               0 if  need to create file \n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os \n",
    "    import video_stuff\n",
    "    numSteps=bagsize\n",
    "    numFeats=no_of_features\n",
    "    try:\n",
    "        dataset_name = pd.read_csv(data_csv,index_col=0)\n",
    "    except:\n",
    "        dataset_name = data_csv\n",
    "    labels = dataset_name['label']\n",
    "\n",
    "\n",
    "    labels=pd.DataFrame(labels)\n",
    "\n",
    "    dataset_name.drop(['video_path','label'],axis=1,inplace=True)\n",
    "    filtered_train = dataset_name\n",
    "    filtered_target = labels\n",
    "    print(filtered_train.shape)\n",
    "    print(filtered_target.shape)\n",
    "    y = filtered_target.values.reshape(int(len(filtered_target)/bagsize),bagsize)    #input bagsize\n",
    "    x = filtered_train.values                                                                                    #no_of_features=540\n",
    "    x = x.reshape(int(len(x) / bagsize),bagsize, no_of_features)  \n",
    "    print(x.shape)                         #(Bags, Timesteps, Features)\n",
    "    one_hot_list = []\n",
    "    for i in range(len(y)):\n",
    "        one_hot_list.append(set(y[i]).pop())\n",
    "\n",
    "    categorical_y_ver = one_hot_list\n",
    "    categorical_y_ver = np.array(categorical_y_ver)\n",
    "\n",
    "    def one_hot(y, numOutput):\n",
    "        y = np.reshape(y, [-1])\n",
    "        ret = np.zeros([y.shape[0], numOutput])\n",
    "        for i, label in enumerate(y):\n",
    "            ret[i, label] = 1\n",
    "        return ret\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pathlib\n",
    "\n",
    "    x_train_val_combined, x_test, y_train_val_combined, y_test = train_test_split(x, categorical_y_ver, test_size=0.20, random_state=13)                                                                                        #extractedDir = '/home/adithyapa4444_gmail_com/'\n",
    "    timesteps = x_train_val_combined.shape[-2] #125\n",
    "    feats = x_train_val_combined.shape[-1]  #9\n",
    "    trainSize = int(x_train_val_combined.shape[0]*0.9) #6566\n",
    "    x_train, x_val = x_train_val_combined[:trainSize], x_train_val_combined[trainSize:] \n",
    "    y_train, y_val = y_train_val_combined[:trainSize], y_train_val_combined[trainSize:]\n",
    "    \n",
    "    # normalization\n",
    "    x_train = np.reshape(x_train, [-1, feats])\n",
    "    mean = np.mean(x_train, axis=0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "\n",
    "    # normalize train\n",
    "    x_train = x_train - mean\n",
    "    x_train = x_train / std\n",
    "    x_train = np.reshape(x_train, [-1, timesteps, feats])\n",
    "\n",
    "    # normalize val\n",
    "    x_val = np.reshape(x_val, [-1, feats])\n",
    "    x_val = x_val - mean\n",
    "    x_val = x_val / std\n",
    "    x_val = np.reshape(x_val, [-1, timesteps, feats])\n",
    "\n",
    "    # normalize test\n",
    "    x_test = np.reshape(x_test, [-1, feats])\n",
    "    x_test = x_test - mean\n",
    "    x_test = x_test / std\n",
    "    x_test = np.reshape(x_test, [-1, timesteps, feats])\n",
    "\n",
    "    # shuffle test, as this was remaining\n",
    "    idx = np.arange(len(x_test))\n",
    "    np.random.shuffle(idx)\n",
    "    x_test = x_test[idx]\n",
    "    y_test = y_test[idx]\n",
    "    extractedDir += '/'\n",
    "    if raw_create==0:\n",
    "        if (os.path.isdir(extractedDir+'RAW')):\n",
    "            print(\"A raw file already exsist in this directory\")\n",
    "            return\n",
    "        else:\n",
    "            numOutput = numClass\n",
    "            y_train = one_hot(y_train, numOutput)\n",
    "            y_val = one_hot(y_val, numOutput)\n",
    "            y_test = one_hot(y_test, numOutput)\n",
    "            pathlib.Path(extractedDir + 'RAW').mkdir(parents=True, exist_ok = True)\n",
    "            np.save(extractedDir + \"RAW/x_train\", x_train)\n",
    "            np.save(extractedDir + \"RAW/y_train\", y_train)\n",
    "            np.save(extractedDir + \"RAW/x_test\", x_test)\n",
    "            np.save(extractedDir + \"RAW/y_test\", y_test)\n",
    "            np.save(extractedDir + \"RAW/x_val\", x_val)\n",
    "            np.save(extractedDir + \"RAW/y_val\", y_val)\n",
    "    numOutput = numClass\n",
    "    y_train = one_hot(y_train, numOutput)\n",
    "    y_val = one_hot(y_val, numOutput)\n",
    "    y_test = one_hot(y_test, numOutput)\n",
    "    data = {\n",
    "        'x_train' : x_train, \n",
    "        'y_train' : y_train, \n",
    "        'x_val' : x_val, \n",
    "        'y_val' : y_val, \n",
    "        'x_test' : x_test, \n",
    "        'y_test' : y_test\n",
    "    }\n",
    "            \n",
    "    def loadData(dirname):\n",
    "        x_train = np.load(dirname + '/' + 'x_train.npy')\n",
    "        y_train = np.load(dirname + '/' + 'y_train.npy')\n",
    "        x_test = np.load(dirname + '/' + 'x_test.npy')\n",
    "        y_test = np.load(dirname + '/' + 'y_test.npy')\n",
    "        x_val = np.load(dirname + '/' + 'x_val.npy')\n",
    "        y_val = np.load(dirname + '/' + 'y_val.npy')\n",
    "        return x_train, y_train, x_test, y_test, x_val, y_val\n",
    "\n",
    "    def bagData(X, Y, subinstanceLen, subinstanceStride,numClass,numSteps,numFeats):\n",
    "        '''\n",
    "        Takes x and y of shape\n",
    "        [-1, 128, 9] and [-1, 6] respectively and converts it into bags of instances.\n",
    "        returns [-1, numInstance, ]\n",
    "        '''\n",
    "        #numClass = 2\n",
    "        #numSteps = 24 # Window length\n",
    "        #numFeats = 540 # Number of features\n",
    "        print(\"subinstanceLen:\",subinstanceLen,\"\\nsubinstanceStride :\", subinstanceStride,\"\\nnumClass:\",numClass,\"\\nnumSteps\",numSteps,\"\\nnumFeats\",numFeats)\n",
    "        print(\"X Shape :\",X.shape)\n",
    "        print(\"Y Shape :\",Y.shape)\n",
    "        assert X.ndim == 3\n",
    "        assert X.shape[1] == numSteps\n",
    "        assert X.shape[2] == numFeats\n",
    "        assert subinstanceLen <= numSteps\n",
    "        assert subinstanceLen > 0 # subinstance length = Number of readings for which the class signature occurs\n",
    "        assert subinstanceStride <= numSteps  \n",
    "        assert subinstanceStride >= 0 \n",
    "        assert len(X) == len(Y)\n",
    "        assert Y.ndim == 2\n",
    "        assert Y.shape[1] == numClass\n",
    "        x_bagged = []\n",
    "        y_bagged = []\n",
    "        for i, point in enumerate(X[:, :, :]):\n",
    "            instanceList = []\n",
    "            start = 0\n",
    "            end = subinstanceLen\n",
    "            while True:\n",
    "                x = point[start:end, :]\n",
    "                if len(x) < subinstanceLen:\n",
    "                    x_ = np.zeros([subinstanceLen, x.shape[1]])\n",
    "                    x_[:len(x), :] = x[:, :]\n",
    "                    x = x_\n",
    "                instanceList.append(x)\n",
    "                if end >= numSteps:\n",
    "                    break\n",
    "                start += subinstanceStride\n",
    "                end += subinstanceStride\n",
    "            bag = np.array(instanceList)\n",
    "            numSubinstance = bag.shape[0]\n",
    "            label = Y[i]\n",
    "            label = np.argmax(label)\n",
    "            labelBag = np.zeros([numSubinstance, numClass])\n",
    "            labelBag[:, label] = 1\n",
    "            x_bagged.append(bag)\n",
    "            label = np.array(labelBag)\n",
    "            y_bagged.append(label)\n",
    "        return np.array(x_bagged), np.array(y_bagged)\n",
    "                                                                                                            #sourceDir, outDir\n",
    "    def makeEMIData(subinstanceLen, subinstanceStride, data, sourceDir, outDir,numClass,numSteps,numFeats):\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = loadData(sourceDir)\n",
    "        try:\n",
    "            x_train, y_train = bagData(x_train, y_train, subinstanceLen, subinstanceStride,numClass,numSteps,numFeats)\n",
    "    #         np.save(outDir + '/x_train.npy', x)\n",
    "    #         np.save(outDir + '/y_train.npy', y)\n",
    "            print('Num train %d' % len(x))\n",
    "            x_test, y_test = bagData(x_test, y_test, subinstanceLen, subinstanceStride,numClass,numSteps,numFeats)\n",
    "    #         np.save(outDir + '/x_test.npy', x)\n",
    "    #         np.save(outDir + '/y_test.npy', y)\n",
    "            print('Num test %d' % len(x))\n",
    "            x_val, y_val = bagData(x_val, y_val, subinstanceLen, subinstanceStride,numClass,numSteps,numFeats)\n",
    "            print('Num val %d' % len(x))\n",
    "        except:\n",
    "            x_train, y_train = bagData(data['x_train'], data['y_train'], subinstanceLen, subinstanceStride,numClass,numSteps,numFeats)\n",
    "    #         np.save(outDir + '/x_train.npy', x)\n",
    "    #         np.save(outDir + '/y_train.npy', y)\n",
    "            print('Num train %d' % len(x))\n",
    "            x_test, y_test = bagData(data['x_test'], data['y_test'], subinstanceLen, subinstanceStride,numClass,numSteps,numFeats)\n",
    "    #         np.save(outDir + '/x_test.npy', x)\n",
    "    #         np.save(outDir + '/y_test.npy', y)\n",
    "            print('Num test %d' % len(x))\n",
    "            x_val, y_val = bagData(data['x_val'], data['y_val'], subinstanceLen, subinstanceStride,numClass,numSteps,numFeats)\n",
    "            print('Num val %d' % len(x))\n",
    "#         np.save(outDir + '/x_val.npy', x)\n",
    "#         np.save(outDir + '/y_val.npy', y)\n",
    "        \n",
    "        return(x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "\n",
    "    extractedDir = '/home/adithyapa4444_gmail_com'\n",
    "    rawDir = extractedDir + 'RAW'\n",
    "    sourceDir = rawDir\n",
    "    from os import mkdir\n",
    "    # WHEN YOU CHANGE THE ABOVE - CREATE A FOLDER \n",
    "    if (not(os.path.isdir(extractedDir+'/'+str(subinstanceLen)+'_'+str(subinstanceStride)))):\n",
    "        mkdir(extractedDir+'/'+str(subinstanceLen)+'_'+str(subinstanceStride))  \n",
    "\n",
    "    outDir = extractedDir + '%d_%d' % (subinstanceLen, subinstanceStride)\n",
    "    return(makeEMIData(subinstanceLen, subinstanceStride, data, sourceDir, outDir,numClass,numSteps,numFeats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data that is loaded is in a format compatible with EMI-RNN. Change **path** to the directory containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:52:00.022110Z",
     "start_time": "2019-04-30T09:51:59.925101Z"
    }
   },
   "outputs": [],
   "source": [
    "path=\"/home/adithyapa4444_gmail_com/UCF_Crime/Full_UCrime_4d/64_16/\"\n",
    "# x_train, y_train = np.load(path + 'x_train.npy'), np.load(path + 'y_train.npy')\n",
    "# x_test, y_test = np.load(path + 'x_test.npy'), np.load(path + 'y_test.npy')\n",
    "# x_val, y_val = np.load(path + 'x_val.npy'), np.load(path + 'y_val.npy')\n",
    "\n",
    "# BAG_TEST, BAG_TRAIN, BAG_VAL represent bag_level labels. These are used for the label update\n",
    "# step of EMI/MI RNN\n",
    "BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "NUM_SUBINSTANCE = x_train.shape[1]\n",
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_val.shape)\n",
    "print(\"y_test shape is:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[:1]\n",
    "y_test = y_test[:1]\n",
    "np.save(path + 'x_test_pi',x_test)\n",
    "np.save(path + 'y_test_pi',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:52:00.604049Z",
     "start_time": "2019-04-30T09:52:00.589634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the linear secondary classifier\n",
    "def createExtendedGraph(self, baseOutput, *args, **kwargs):\n",
    "    W1 = tf.Variable(np.random.normal(size=[NUM_HIDDEN, NUM_OUTPUT]).astype('float32'), name='W1')\n",
    "    B1 = tf.Variable(np.random.normal(size=[NUM_OUTPUT]).astype('float32'), name='B1')\n",
    "    y_cap = tf.add(tf.tensordot(baseOutput, W1, axes=1), B1, name='y_cap_tata')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "    \n",
    "def addExtendedAssignOps(self, graph, W_val=None, B_val=None):\n",
    "    W1 = graph.get_tensor_by_name('W1:0')\n",
    "    B1 = graph.get_tensor_by_name('B1:0')\n",
    "    W1_op = tf.assign(W1, W_val)\n",
    "    B1_op = tf.assign(B1, B_val)\n",
    "    self.assignOps.extend([W1_op, B1_op])\n",
    "\n",
    "def restoreExtendedGraph(self, graph, *args, **kwargs):\n",
    "    y_cap = graph.get_tensor_by_name('y_cap_tata:0')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "    \n",
    "def feedDictFunc(self, keep_prob=None, inference=False, **kwargs):\n",
    "    if inference is False:\n",
    "        feedDict = {self._emiGraph.keep_prob: keep_prob}\n",
    "    else:\n",
    "        feedDict = {self._emiGraph.keep_prob: 1.0}\n",
    "    return feedDict\n",
    "\n",
    "EMI_FastGRNN._createExtendedGraph = createExtendedGraph\n",
    "EMI_FastGRNN._restoreExtendedGraph = restoreExtendedGraph\n",
    "EMI_FastGRNN.addExtendedAssignOps = addExtendedAssignOps\n",
    "\n",
    "def earlyPolicy_minProb(instanceOut, minProb, **kwargs):\n",
    "    assert instanceOut.ndim == 2\n",
    "    classes = np.argmax(instanceOut, axis=1)\n",
    "    prob = np.max(instanceOut, axis=1)\n",
    "    index = np.where(prob >= minProb)[0]\n",
    "    if len(index) == 0:\n",
    "        assert (len(instanceOut) - 1) == (len(classes) - 1)\n",
    "        return classes[-1], len(instanceOut) - 1\n",
    "    index = index[0]\n",
    "    return classes[index], index\n",
    "\n",
    "\n",
    "if USE_DROPOUT is True:\n",
    "    EMI_Driver.feedDictFunc = feedDictFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T09:34:06.288012Z",
     "start_time": "2018-08-19T09:34:06.285286Z"
    }
   },
   "source": [
    "## 1. Initializing a New Computation Graph\n",
    "\n",
    "In the most common use cases, a new EMI-RNN graph would be created and trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:52:10.701762Z",
     "start_time": "2019-04-30T09:52:02.074816Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT)\n",
    "emiLSTM = EMI_FastGRNN(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS, wRank=WRANK, uRank=URANK, \n",
    "                              gate_non_linearity=GATE_NL, update_non_linearity=UPDATE_NL, useDropout=USE_DROPOUT)\n",
    "emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy')\n",
    "\n",
    "# Construct the graph\n",
    "g1 = tf.Graph()    \n",
    "with g1.as_default():\n",
    "    x_batch, y_batch = inputPipeline()\n",
    "    y_cap = emiLSTM(x_batch)\n",
    "    emiTrainer(y_cap, y_batch)\n",
    "    \n",
    "with g1.as_default():\n",
    "    emiDriver = EMI_Driver(inputPipeline, emiLSTM, emiTrainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize a new session with this graph and train a model. The saved model will be used later for restoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T06:25:20.220063Z",
     "start_time": "2019-04-25T06:25:19.987538Z"
    }
   },
   "outputs": [],
   "source": [
    "emiDriver.initializeSession(g1)\n",
    "y_updated, modelStats = emiDriver.run(numClasses=NUM_OUTPUT, x_train=x_train,\n",
    "                                     y_train=y_train, bag_train=BAG_TRAIN,\n",
    "                                     x_val=x_val, y_val=y_val, bag_val=BAG_VAL,\n",
    "                                     numIter=NUM_ITER, keep_prob=KEEP_PROB,\n",
    "                                     numRounds=NUM_ROUNDS, batchSize=BATCH_SIZE,\n",
    "                                     numEpochs=NUM_EPOCHS, modelPrefix=MODEL_PREFIX,\n",
    "                                     fracEMI=0.5, updatePolicy='top-k', k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output above indicates, the last restored model is `/tmp/model-lstm-1001`. That is, with `MODEL_PREFIX = '/tmp/model-lstm'` and `GLOBAL_STEP=1001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T11:48:33.294431Z",
     "start_time": "2018-08-19T11:48:32.897376Z"
    }
   },
   "outputs": [],
   "source": [
    "def earlyPolicy_minProb(instanceOut, minProb, **kwargs):\n",
    "    assert instanceOut.ndim == 2\n",
    "    classes = np.argmax(instanceOut, axis=1)\n",
    "    prob = np.max(instanceOut, axis=1)\n",
    "    index = np.where(prob >= minProb)[0]\n",
    "    if len(index) == 0:\n",
    "        assert (len(instanceOut) - 1) == (len(classes) - 1)\n",
    "        return classes[-1], len(instanceOut) - 1\n",
    "    index = index[0]\n",
    "    return classes[index], index\n",
    "\n",
    "emiDriver.initializeSession(g1)\n",
    "\n",
    "k = 2\n",
    "predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                               minProb=0.99, keep_prob=1.0)\n",
    "bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "\n",
    "k = 2\n",
    "predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb, minProb=0.99)\n",
    "bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "print('Additional savings: %f' % getEarlySaving(predictionStep, NUM_TIMESTEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading a Saved Graph into EMI-Driver\n",
    "\n",
    "We will reset the computation graph, load a saved graph into the current `EMI_Driver` and verify its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T06:24:09.713351Z",
     "start_time": "2019-04-25T06:24:09.638610Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "emiDriver.initializeSession(g1)\n",
    "emiDriver.loadSavedGraphToNewSession(MODEL_PREFIX , 1037)\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T06:24:09.713351Z",
     "start_time": "2019-04-25T06:24:09.638610Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k=3\n",
    "import time\n",
    "start = time.time()\n",
    "predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                               minProb=0.99, keep_prob=1.0)\n",
    "bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "print(time.time() - start)\n",
    "print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "def getEarlySaving(predictionStep, numTimeSteps, returnTotal=False):\n",
    "    predictionStep = predictionStep + 1\n",
    "    predictionStep = np.reshape(predictionStep, -1)\n",
    "    totalSteps = np.sum(predictionStep)\n",
    "    maxSteps = len(predictionStep) * numTimeSteps\n",
    "    savings = 1.0 - (totalSteps / maxSteps)\n",
    "    if returnTotal:\n",
    "        return savings, totalSteps\n",
    "    return savings\n",
    "print('Additional savings: %f' % getEarlySaving(predictionStep, NUM_TIMESTEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T06:24:09.713351Z",
     "start_time": "2019-04-25T06:24:09.638610Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing using a Saved Graph\n",
    "\n",
    "Here we will construct a new computation graph, but will use a previously trained model to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T12:07:09.616748Z",
     "start_time": "2019-04-24T12:07:09.596906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making sure the old graph and sessions are closed\n",
    "sess = emiDriver.getCurrentSession()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `GraphManager` to load the saved graph and load it into a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:52:19.568739Z",
     "start_time": "2019-04-30T09:52:10.703663Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "graphManager = utils.GraphManager()\n",
    "graph = graphManager.loadCheckpoint(sess, MODEL_PREFIX, globalStep=1037)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the forward graph as before, but provide the loaded `graph` as an argument to `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:52:20.570380Z",
     "start_time": "2019-04-30T09:52:19.571022Z"
    }
   },
   "outputs": [],
   "source": [
    "inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT, graph=graph)\n",
    "emiLSTM = EMI_FastGRNN(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS, wRank=WRANK, uRank=URANK, \n",
    "                               gate_non_linearity=GATE_NL, update_non_linearity=UPDATE_NL, useDropout=USE_DROPOUT)\n",
    "emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy', graph=graph)\n",
    "\n",
    "g1 = graph\n",
    "with g1.as_default():\n",
    "    x_batch, y_batch = inputPipeline()\n",
    "    y_cap = emiLSTM(x_batch)\n",
    "    emiTrainer(y_cap, y_batch)\n",
    "    \n",
    "with g1.as_default():\n",
    "    emiDriver = EMI_Driver(inputPipeline, emiLSTM, emiTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `EMI_Driver` know that we already have a session in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:52:20.574716Z",
     "start_time": "2019-04-30T09:52:20.572193Z"
    }
   },
   "outputs": [],
   "source": [
    "emiDriver.setSession(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:52:34.913965Z",
     "start_time": "2019-04-30T09:52:32.795936Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# emiDriver.initializeSession(g1)\n",
    "# emiDriver.loadSavedGraphToNewSession(MODEL_PREFIX, 1007)\n",
    "k = 2\n",
    "predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                            minProb=0.99, keep_prob=1.0)\n",
    "bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                                minProb=0.99, keep_prob=1.0)\n",
    "    bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "\n",
    "    end = time.time()\n",
    "y_test_bag = np.argmax(y_test[:, 0, :], axis=1)\n",
    "metrics_dict = {'time_run': end - start,\n",
    "                'y_pred': bagPredictions,\n",
    "                'y_test': y_test_bag,\n",
    "                'classification_report': classification_report(bagPredictions, y_test_bag),\n",
    "                'confusion_matrix': confusion_matrix(bagPredictions, y_test_bag)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics_dict:\n",
    "    print('\\n')\n",
    "    print(metric)\n",
    "    print(metrics_dict[metric])\n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Restoring from Numpy Matrices\n",
    "\n",
    "We first extract the model matrices from the graph and dump it into `.npy` files. Then we load it back again and initialize a new graph with these matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T11:48:44.379901Z",
     "start_time": "2018-08-19T11:48:44.326706Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "W1 = graph.get_tensor_by_name('W1:0')\n",
    "B1 = graph.get_tensor_by_name('B1:0')\n",
    "allVars = emiLSTM.varList + [W1, B1]\n",
    "sess = emiDriver.getCurrentSession()\n",
    "allVars = sess.run(allVars)\n",
    "\n",
    "base = '/tmp/models/'\n",
    "np.save(base + 'kernel.npy', allVars[0])\n",
    "np.save(base + 'bias.npy', allVars[1])\n",
    "np.save(base + 'W1.npy', allVars[2])\n",
    "np.save(base + 'B1.npy', allVars[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T11:16:55.606967Z",
     "start_time": "2018-08-19T11:16:55.535964Z"
    }
   },
   "source": [
    "Reset the current session and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T11:48:44.389724Z",
     "start_time": "2018-08-19T11:48:44.381802Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = emiDriver.getCurrentSession()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the numpy matrices that will be used to initialize the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T11:48:44.442241Z",
     "start_time": "2018-08-19T11:48:44.391384Z"
    }
   },
   "outputs": [],
   "source": [
    "base = '/home/iot/Documents/EdgeML-master/tf/examples/EMI-RNN/GRNN model'\n",
    "kernel = np.load(base + 'kernel.npy')\n",
    "bias = np.load(base + 'bias.npy')\n",
    "W = np.load(base + 'W1.npy')\n",
    "B = np.load(base + 'B1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T11:38:36.118298Z",
     "start_time": "2018-08-19T11:38:36.113810Z"
    }
   },
   "source": [
    "Proceed with graph construction as normally done, except that we add the requisite assignment operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T11:48:51.378377Z",
     "start_time": "2018-08-19T11:48:44.444182Z"
    }
   },
   "outputs": [],
   "source": [
    "inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS,\n",
    "                                 NUM_OUTPUT)\n",
    "emiLSTM = EMI_Fast(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS,\n",
    "                        forgetBias=FORGET_BIAS, useDropout=USE_DROPOUT)\n",
    "emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x_batch, y_batch = inputPipeline()\n",
    "    y_cap = emiLSTM(x_batch)\n",
    "    emiTrainer(y_cap, y_batch)\n",
    "    # Add the assignment operations\n",
    "    emiLSTM.addBaseAssignOps(graph, [kernel, bias])\n",
    "    emiLSTM.addExtendedAssignOps(graph, W, B)\n",
    "    # Setup the driver. You can run the initializations manually as well\n",
    "    emiDriver = EMI_Driver(inputPipeline, emiLSTM, emiTrainer)\n",
    "\n",
    "emiDriver.initializeSession(graph)\n",
    "# Run the assignment operations\n",
    "sess = emiDriver.getCurrentSession()\n",
    "sess.run(emiLSTM.assignOps)\n",
    "\n",
    "k = 2\n",
    "predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test,\n",
    "                                                               earlyPolicy_minProb,\n",
    "                                                               minProb=0.99,\n",
    "                                                               keep_prob=1.0)\n",
    "bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k,\n",
    "                                             numClass=NUM_OUTPUT)\n",
    "print('PART IV: Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions ==\n",
    "                                                        BAG_TEST).astype(int))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
